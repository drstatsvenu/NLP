{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                1288300   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,351\n",
      "Trainable params: 1,288,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Sm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4571 - acc: 0.7980\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0554 - acc: 0.9945\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.3969e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.2624e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ab9c780>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\t# remove punctuation from each word\n",
    "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "\t# load the doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "\tlines = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load and clean the doc\n",
    "\t\tline = doc_to_line(path, vocab)\n",
    "\t\t# add to list\n",
    "\t\tlines.append(line)\n",
    "\treturn lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "\t# load documents\n",
    "\tneg = process_docs('C:/Users/Sm/Desktop/ACE/neg', vocab)\n",
    "\tpos = process_docs('C:/Users/Sm/Desktop/ACE/pos', vocab)\n",
    "\tdocs = neg + pos\n",
    "\t# prepare labels\n",
    "\tlabels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "\treturn docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "\t# define network\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile network\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize defined model\n",
    "\tmodel.summary()\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model\n",
    "\n",
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "\t# clean\n",
    "\ttokens = clean_doc(review)\n",
    "\t# filter by vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\t# convert to line\n",
    "\tline = ' '.join(tokens)\n",
    "\t# encode\n",
    "\tencoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
    "\t# predict sentiment\n",
    "\tyhat = model.predict(encoded, verbose=0)\n",
    "\t# retrieve predicted percentage and label\n",
    "\tpercent_pos = yhat[0,0]\n",
    "\tif round(percent_pos) == 0:\n",
    "\t\treturn (1-percent_pos), 'NEGATIVE'\n",
    "\treturn percent_pos, 'POSITIVE'\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab)\n",
    "test_docs, ytest = load_clean_dataset(vocab)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
    "# define network\n",
    "n_words = Xtrain.shape[1]\n",
    "model = define_model(n_words)\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Best movie ever! It was great, I recommend it.]\n",
      "Sentiment: POSITIVE (56.423%)\n",
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (64.940%)\n"
     ]
    }
   ],
   "source": [
    "# test positive text\n",
    "text = 'Best movie ever! It was great, I recommend it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [I didnt like this moviet]\n",
      "Sentiment: NEGATIVE (62.399%)\n"
     ]
    }
   ],
   "source": [
    "text = 'I didnt like this moviet'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
